{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embeddings model\n",
    "\n",
    "Let's start by initializing our embeddings model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can embed a list of texts with a single call to `embed_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I like to eat bananas.\",\n",
    "    \"The dog ate my homework.\",\n",
    "    \"It is sunny outside.\",\n",
    "]\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 1536\n",
      "First 10 values: [-0.014765617668302448, -0.02597355510632489, 0.014292918368914687, 0.003741162189046819, 0.006266368650924281, -0.012140895429651924, -0.01944284730227332, -0.028685352281771787, -0.0017337472216025901, -0.011425627573981148]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions: {len(embeddings[0])}\")\n",
    "print(f\"First 10 values: {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also embed a single text with `embed_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005356039053113689,\n",
       " -0.000552351843204413,\n",
       " 0.03886180874395785,\n",
       " -0.002978327498861753,\n",
       " -0.008890430821354354]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query(\n",
    "    \"What was the name mentioned in the conversation?\"\n",
    ")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.\n",
    "\n",
    "We will use the `chroma` vector database, which runs on your local machine as a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai\\albedo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# Load the raw text from a PDF file\n",
    "documents = PyPDFLoader(\"./data/drr-qa-system.pdf\").load()\n",
    "\n",
    "# Split the text into chunks of 1000 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 0,\n",
    "    # length_function = len,\n",
    "    # is_separator_regex = False,\n",
    "    # separators=[\" \", \"\\n\", \"\\n\\n\"],\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    ")\n",
    "\n",
    "# Store the embeddings in a vector store\n",
    "db = Chroma.from_documents(documents, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing A Question-Answering System for\n",
      "Community-Centric Disaster Preparedness\n",
      "Rixdon Niño R. Mape\n",
      "Project Description\n",
      "In times of disaster, the availability of precise and locally-tailored information can be the difference\n",
      "between safety and peril. One fundamental problem communities face is obtaining up-to-the-minute,\n",
      "relevant information specific to their needs. Traditional question-answering systems excel at drawing\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search for similar documents using a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appropriate information to users. These LLMs are akin to sophisticated software that can compre-\n",
      "hend human language and generate responses as a person might. However, LLMs, despite their in-\n",
      "telligence, are prone to a critical limitation—they may generate plausible but false information when\n",
      "faced with queries beyond their training scope, a phenomenon known as ‘hallucination’ . In the context\n",
      "of disaster response, such misinformation could lead to dire consequences.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is hallucination?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to do a search for documents similar to a given embedding vector using `similarity_search_by_vector` which accepts an embedding vector as a parameter instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appropriate information to users. These LLMs are akin to sophisticated software that can compre-\n",
      "hend human language and generate responses as a person might. However, LLMs, despite their in-\n",
      "telligence, are prone to a critical limitation—they may generate plausible but false information when\n",
      "faced with queries beyond their training scope, a phenomenon known as ‘hallucination’ . In the context\n",
      "of disaster response, such misinformation could lead to dire consequences.\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embedding_function.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Add section for passing a Chroma Client into Langchain\n",
    "\n",
    "**TODO**: Add section for filtering result by metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
