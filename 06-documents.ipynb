{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "Use document loaders to load data from a source as `Document`'s. A `Document` is a piece of text and associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.\n",
    "\n",
    "The simplest loader reads in a file as text and places it all into one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Social Cancer: A Complete English Version of Noli Me Tangere\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Guten...\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/the-social-cancer.txt\")\n",
    "pages = loader.load()  # Returns a list of `Document` objects\n",
    "print(f\"{pages[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PDF Files Using PyPDF\n",
    "\n",
    "This covers how to load `PDF` documents into the `Document` format that we use downstream. Load `PDF` using `pypdf` into array of documents, where each document contains the page content and metadata with page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./data/neuron.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Understanding Neurons: The Building Blocks of\\nthe Nervous System\\nIntroduction\\nNeurons are the core components of the nervous system, responsible for carrying\\nmessages throughout the body. These specialized cells are the main players in the\\nbrain and spinal cord of the central nervous system, as well as the nerves that run\\nthroughout our body in the peripheral nervous system.\\nNeurons are the building blocks of the nervous system. They are responsible for\\ncarrying messages throughout the body. These specialized cells are the main play-\\ners in the brain and spinal cord of the central nervous system, as well as the nerves\\nthat run throughout our body in the peripheral nervous system.\\nWhat are Neurons?\\nNeurons are cells designed to transmit information. They are unique in their shape\\nand function, optimized for sending and receiving signals. Neurons are different\\nfrom other cells in the body because of their ability to communicate through electri-\\ncal and chemical signals.\\nThe human brain contains approximately 86 billion neurons, and each one can con-\\nnect to thousands of other neurons, forming a complex network of communication.\\nThese connections allow neurons to work together to process information and exe-\\ncute responses.\\n1', metadata={'source': './data/neuron.pdf', 'page': 0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the content of the PDF as context for a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Transformers\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitters\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "- Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "- Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "- Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 108787\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "loader = TextLoader(\"./data/the-social-cancer.txt\")\n",
    "pages = loader.load()\n",
    "word_count = len(pages[0].page_content.split())\n",
    "print(f\"Word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2755\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pages[0].page_content])\n",
    "print(f\"Number of chunks: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk: 'The Social Cancer: A Complete English Version of Noli Me Tangere'\n",
      "\n",
      "Second chunk: 'This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online'\n",
      "\n",
      "Third chunk: 'at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First chunk: '{texts[0].page_content}'\\n\")\n",
    "print(f\"Second chunk: '{texts[1].page_content}'\\n\")\n",
    "print(f\"Third chunk: '{texts[2].page_content}'\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
